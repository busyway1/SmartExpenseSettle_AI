"""
PDF íŒŒì‹± ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ ì—”ì§„

ğŸš€ Upstage Document Parse ë©”ì¸ ì—”ì§„ + ë‹¤ì¤‘ ë°±ì—… ì—”ì§„
âš¡ 0.6ì´ˆ/í˜ì´ì§€ ì´ˆê³ ì† ì²˜ë¦¬
ğŸ¯ 93.48% TEDS ì •í™•ë„ ë³´ì¥
ğŸ”„ ìë™ í´ë°± ì‹œìŠ¤í…œ (Upstage â†’ PyMuPDF â†’ pdfplumber â†’ Tesseract OCR)
"""

import asyncio
import os
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import logging

# PDF ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
from pdf2image import convert_from_path

# HTTP í´ë¼ì´ì–¸íŠ¸
import requests
import json

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from .models import (
    ExtractionEngine, 
    DocumentType, 
    PDFProcessingResult,
    ProcessingStatus,
    DocumentDetection
)
from .utils import validate_pdf_file, get_file_info, clean_text

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PDFParsingEngine:
    """
    ë‹¤ì¤‘ ì—”ì§„ì„ í™œìš©í•œ PDF íŒŒì‹± ì‹œìŠ¤í…œ
    
    ì—”ì§„ ìš°ì„ ìˆœìœ„:
    1. ğŸ¥‡ Upstage Document Parse (ë©”ì¸)
    2. ğŸ¥ˆ PyMuPDF (ë¹ ë¥¸ ì²˜ë¦¬)
    3. ğŸ¥‰ pdfplumber (ì •í™•í•œ ë ˆì´ì•„ì›ƒ)
    4. ğŸ”§ Tesseract OCR (ìµœí›„ ìˆ˜ë‹¨)
    """
    
    def __init__(self, upstage_api_key: str = None, verbose: bool = False):
        """
        Args:
            upstage_api_key: Upstage API í‚¤
            verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
        """
        self.verbose = verbose
        self.upstage_api_key = upstage_api_key or os.getenv('UPSTAGE_API_KEY')
        
        # ì—”ì§„ë³„ í†µê³„
        self.engine_stats = {
            engine: {"success": 0, "failure": 0, "total_time": 0.0}
            for engine in ExtractionEngine
        }
        
        if self.verbose:
            logger.info("ğŸš€ PDF íŒŒì‹± ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def extract_text_from_pdf(
        self, 
        file_path: str, 
        preferred_engine: ExtractionEngine = ExtractionEngine.UPSTAGE
    ) -> Tuple[str, ExtractionEngine, float]:
        """
        PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìë™ í´ë°± ì§€ì›)
        
        Args:
            file_path: PDF íŒŒì¼ ê²½ë¡œ
            preferred_engine: ì„ í˜¸í•˜ëŠ” ì¶”ì¶œ ì—”ì§„
            
        Returns:
            (ì¶”ì¶œëœ_í…ìŠ¤íŠ¸, ì‚¬ìš©ëœ_ì—”ì§„, ì²˜ë¦¬_ì‹œê°„)
        """
        start_time = datetime.now()
        
        # PDF íŒŒì¼ ê²€ì¦
        validation_result = validate_pdf_file(file_path)
        if not validation_result["is_valid"]:
            raise ValueError(f"PDF íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {validation_result['error']}")
        
        if self.verbose:
            logger.info(f"ğŸ“„ PDF íŒŒì‹± ì‹œì‘: {Path(file_path).name}")
        
        # ì—”ì§„ ì‹œë„ ìˆœì„œ ê²°ì •
        engine_order = self._get_engine_order(preferred_engine)
        
        last_error = None
        
        for engine in engine_order:
            try:
                if self.verbose:
                    logger.info(f"ğŸ”§ {engine.value} ì—”ì§„ìœ¼ë¡œ ì‹œë„ ì¤‘...")
                
                text = await self._extract_with_engine(file_path, engine)
                
                # ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸ (ë” ê´€ëŒ€í•˜ê²Œ)
                min_length = 20 if engine == ExtractionEngine.UPSTAGE else 50
                if text and len(text.strip()) > min_length:
                    processing_time = (datetime.now() - start_time).total_seconds()
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    self.engine_stats[engine]["success"] += 1
                    self.engine_stats[engine]["total_time"] += processing_time
                    
                    if self.verbose:
                        logger.info(f"âœ… {engine.value} ì„±ê³µ ({processing_time:.2f}ì´ˆ)")
                    
                    return clean_text(text), engine, processing_time
                else:
                    if self.verbose:
                        logger.warning(f"âš ï¸ {engine.value} í…ìŠ¤íŠ¸ ë¶€ì¡± (ê¸¸ì´: {len(text) if text else 0}, ìµœì†Œ: {min_length})")
                        
            except Exception as e:
                last_error = e
                self.engine_stats[engine]["failure"] += 1
                
                if self.verbose:
                    logger.error(f"âŒ {engine.value} ì‹¤íŒ¨: {str(e)}")
                
                continue
        
        # ëª¨ë“  ì—”ì§„ ì‹¤íŒ¨
        processing_time = (datetime.now() - start_time).total_seconds()
        raise RuntimeError(f"ëª¨ë“  PDF ì¶”ì¶œ ì—”ì§„ ì‹¤íŒ¨. ë§ˆì§€ë§‰ ì˜¤ë¥˜: {last_error}")
    
    async def _extract_with_engine(self, file_path: str, engine: ExtractionEngine) -> str:
        """ê°œë³„ ì—”ì§„ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        
        if engine == ExtractionEngine.UPSTAGE:
            return await self._extract_with_upstage(file_path)
        elif engine == ExtractionEngine.PYMUPDF:
            return await self._extract_with_pymupdf(file_path)
        elif engine == ExtractionEngine.PDFPLUMBER:
            return await self._extract_with_pdfplumber(file_path)
        elif engine == ExtractionEngine.TESSERACT:
            return await self._extract_with_tesseract(file_path)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì—”ì§„: {engine}")
    
    async def _extract_with_upstage(self, file_path: str) -> str:
        """Upstage Document Parse APIë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        
        if not self.upstage_api_key:
            raise ValueError("UPSTAGE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            # Upstage Document Digitization API í˜¸ì¶œ
            url = "https://api.upstage.ai/v1/document-digitization"
            headers = {"Authorization": f"Bearer {self.upstage_api_key}"}
            
            # íŒŒì¼ ì—´ê¸°
            with open(file_path, "rb") as file:
                files = {"document": file}
                data = {
                    "ocr": "force",
                    "base64_encoding": "['table']",
                    "model": "document-parse"
                }
                
                # ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•´ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
                response = await asyncio.to_thread(
                    requests.post, url, headers=headers, files=files, data=data
                )
            
            if response.status_code != 200:
                raise ValueError(f"Upstage API ì˜¤ë¥˜: {response.status_code} - {response.text}")
            
            # JSON ì‘ë‹µ íŒŒì‹±
            result = response.json()
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            full_text = ""
            if "content" in result:
                if isinstance(result["content"], list):
                    # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ê²°í•©
                    text_parts = []
                    for idx, page in enumerate(result["content"]):
                        if "text" in page:
                            text_parts.append(f"--- í˜ì´ì§€ {idx + 1} ---\n{page['text']}")
                        elif isinstance(page, str):
                            text_parts.append(f"--- í˜ì´ì§€ {idx + 1} ---\n{page}")
                    full_text = "\n\n".join(text_parts)
                elif isinstance(result["content"], str):
                    full_text = result["content"]
                else:
                    # ë‹¤ë¥¸ í˜•íƒœì˜ ì‘ë‹µì¼ ê²½ìš° ì „ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                    full_text = str(result["content"])
            elif "text" in result:
                full_text = result["text"]
            else:
                # ì „ì²´ ì‘ë‹µì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë§ˆì§€ë§‰ ì‹œë„)
                full_text = str(result)
            
            if self.verbose:
                pages_count = len(result.get("content", [])) if isinstance(result.get("content"), list) else 1
                logger.info(f"ğŸš€ Upstage: {pages_count}í˜ì´ì§€, {len(full_text)}ì ì¶”ì¶œ")
            
            return full_text
            
        except Exception as e:
            if self.verbose:
                logger.error(f"ğŸš€ Upstage ì˜¤ë¥˜: {str(e)}")
            raise
    
    async def _extract_with_pymupdf(self, file_path: str) -> str:
        """PyMuPDFë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR í¬í•¨)"""
        
        try:
            doc = fitz.open(file_path)
            text_parts = []
            
            if len(doc) == 0:
                doc.close()
                raise ValueError("PDFì— í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                
                # 1ë‹¨ê³„: ì¼ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                text = page.get_text()
                
                # 2ë‹¨ê³„: í…ìŠ¤íŠ¸ê°€ ì—†ê±°ë‚˜ ë§¤ìš° ì ìœ¼ë©´ OCR ì‹œë„
                if not text or len(text.strip()) < 50:
                    try:
                        # í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë Œë”ë§
                        pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x í™•ëŒ€
                        img_data = pix.tobytes("png")
                        
                        # PIL Imageë¡œ ë³€í™˜
                        from PIL import Image
                        import io
                        img = Image.open(io.BytesIO(img_data))
                        
                        # Tesseract OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        import pytesseract
                        ocr_text = pytesseract.image_to_string(
                            img, 
                            lang='kor+eng',
                            config='--psm 6'
                        )
                        
                        if ocr_text and len(ocr_text.strip()) > len(text.strip()):
                            text = ocr_text
                            if self.verbose:
                                logger.info(f"âš¡ PyMuPDF OCR: í˜ì´ì§€ {page_num + 1}ì—ì„œ {len(ocr_text)}ì ì¶”ì¶œ")
                    
                    except Exception as ocr_error:
                        if self.verbose:
                            logger.warning(f"âš¡ PyMuPDF OCR ì‹¤íŒ¨ (í˜ì´ì§€ {page_num + 1}): {str(ocr_error)}")
                
                if text and text.strip():
                    text_parts.append(f"--- í˜ì´ì§€ {page_num + 1} ---\n{text}")
            
            doc.close()
            
            full_text = "\n\n".join(text_parts)
            
            if self.verbose:
                logger.info(f"âš¡ PyMuPDF: {len(text_parts)}í˜ì´ì§€ì—ì„œ {len(full_text)}ì ì¶”ì¶œ")
            
            return full_text
            
        except Exception as e:
            if self.verbose:
                logger.error(f"âš¡ PyMuPDF ì˜¤ë¥˜: {str(e)}")
            raise
    
    async def _extract_with_pdfplumber(self, file_path: str) -> str:
        """pdfplumberë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        
        try:
            text_parts = []
            
            with pdfplumber.open(file_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    text = page.extract_text()
                    
                    if text and text.strip():
                        text_parts.append(f"--- í˜ì´ì§€ {page_num + 1} ---\n{text}")
                        
                        # í…Œì´ë¸”ë„ ì¶”ì¶œ ì‹œë„
                        tables = page.extract_tables()
                        for table_idx, table in enumerate(tables):
                            if table:
                                table_text = "\n".join([
                                    " | ".join([cell or "" for cell in row])
                                    for row in table
                                ])
                                text_parts.append(f"[í…Œì´ë¸” {table_idx + 1}]\n{table_text}")
            
            full_text = "\n\n".join(text_parts)
            
            if self.verbose:
                logger.info(f"ğŸ” pdfplumber: {len(text_parts)}í˜ì´ì§€, {len(full_text)}ì ì¶”ì¶œ")
            
            return full_text
            
        except Exception as e:
            if self.verbose:
                logger.error(f"ğŸ” pdfplumber ì˜¤ë¥˜: {str(e)}")
            raise
    
    async def _extract_with_tesseract(self, file_path: str) -> str:
        """Tesseract OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        
        try:
            # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            images = convert_from_path(
                file_path,
                dpi=300,  # ê³ í•´ìƒë„
                fmt='png'
            )
            
            text_parts = []
            
            for page_num, image in enumerate(images):
                # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (OCR ì •í™•ë„ í–¥ìƒ)
                processed_image = self._preprocess_image_for_ocr(image)
                
                # OCR ì‹¤í–‰ (í•œêµ­ì–´ + ì˜ì–´)
                text = pytesseract.image_to_string(
                    processed_image,
                    lang='kor+eng',  # í•œêµ­ì–´ + ì˜ì–´
                    config='--psm 6'  # ë¸”ë¡ ë‹¨ìœ„ OCR
                )
                
                if text.strip():
                    text_parts.append(f"--- í˜ì´ì§€ {page_num + 1} ---\n{text}")
            
            full_text = "\n\n".join(text_parts)
            
            if self.verbose:
                logger.info(f"ğŸ‘ï¸ Tesseract: {len(images)}í˜ì´ì§€, {len(full_text)}ì ì¶”ì¶œ")
            
            return full_text
            
        except Exception as e:
            if self.verbose:
                logger.error(f"ğŸ‘ï¸ Tesseract ì˜¤ë¥˜: {str(e)}")
            raise
    
    def _preprocess_image_for_ocr(self, image: Image.Image) -> Image.Image:
        """OCR ì •í™•ë„ í–¥ìƒì„ ìœ„í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        if image.mode != 'L':
            image = image.convert('L')
        
        # í•´ìƒë„ í–¥ìƒ (2ë°° í™•ëŒ€)
        width, height = image.size
        image = image.resize((width * 2, height * 2), Image.Resampling.LANCZOS)
        
        # ëŒ€ë¹„ í–¥ìƒ
        from PIL import ImageEnhance
        enhancer = ImageEnhance.Contrast(image)
        image = enhancer.enhance(1.3)
        
        # ì„ ëª…ë„ í–¥ìƒ
        enhancer = ImageEnhance.Sharpness(image)
        image = enhancer.enhance(1.2)
        
        return image
    
    def _get_engine_order(self, preferred_engine: ExtractionEngine) -> List[ExtractionEngine]:
        """ì—”ì§„ ì‹œë„ ìˆœì„œ ê²°ì •"""
        
        # ê¸°ë³¸ ìˆœì„œ
        default_order = [
            ExtractionEngine.UPSTAGE,
            ExtractionEngine.PYMUPDF,
            ExtractionEngine.PDFPLUMBER,
            ExtractionEngine.TESSERACT
        ]
        
        # ì„ í˜¸ ì—”ì§„ì„ ë§¨ ì•ìœ¼ë¡œ
        if preferred_engine in default_order:
            order = [preferred_engine]
            order.extend([engine for engine in default_order if engine != preferred_engine])
            return order
        
        return default_order
    
    def get_engine_statistics(self) -> Dict[str, Any]:
        """ì—”ì§„ë³„ ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        
        stats = {}
        
        for engine, data in self.engine_stats.items():
            total_attempts = data["success"] + data["failure"]
            success_rate = data["success"] / total_attempts if total_attempts > 0 else 0
            avg_time = data["total_time"] / data["success"] if data["success"] > 0 else 0
            
            stats[engine.value] = {
                "success_count": data["success"],
                "failure_count": data["failure"],
                "success_rate": round(success_rate * 100, 1),
                "average_time_seconds": round(avg_time, 2),
                "total_time_seconds": round(data["total_time"], 2)
            }
        
        return stats


class DocumentTypeDetector:
    """
    ë¬¸ì„œ íƒ€ì… ìë™ ê°ì§€ê¸°
    
    ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ë¬´ì—­ë¬¸ì„œ íƒ€ì…ì„ ì‹ë³„í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, verbose: bool = False):
        self.verbose = verbose
        
        # ë¬¸ì„œ íƒ€ì…ë³„ í•µì‹¬ í‚¤ì›Œë“œ
        self.type_keywords = {
            DocumentType.TAX_INVOICE: {
                "primary": ["ì„¸ê¸ˆê³„ì‚°ì„œ", "tax invoice", "ê³µê¸‰ê°€ì•¡", "ì„¸ì•¡", "ë¶€ê°€ê°€ì¹˜ì„¸"],
                "secondary": ["ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸", "í•©ê³„ê¸ˆì•¡", "ê³µê¸‰ì", "ê³µê¸‰ë°›ëŠ”ì", "ë°œí–‰ì¼ì"],
                "negative": []  # ì´ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ í•´ë‹¹ íƒ€ì…ì´ ì•„ë‹˜
            },
            DocumentType.INVOICE: {
                "primary": ["invoice", "commercial invoice", "proforma invoice"],
                "secondary": ["description", "quantity", "unit price", "amount", "total"],
                "negative": ["ì„¸ê¸ˆê³„ì‚°ì„œ", "tax invoice"]
            },
            DocumentType.BILL_OF_LADING: {
                "primary": ["bill of lading", "b/l", "bl"],
                "secondary": ["port of loading", "port of discharge", "vessel", "voyage", "shipper", "consignee"],
                "negative": []
            },
            DocumentType.EXPORT_DECLARATION: {
                "primary": ["ìˆ˜ì¶œì‹ ê³ ", "export declaration", "ì‹ ê³ ë²ˆí˜¸"],
                "secondary": ["ì„¸ë²ˆ", "hs code", "ëª©ì êµ­", "ì ì¬í•­", "ì†¡í’ˆì¥"],
                "negative": []
            },
            DocumentType.TRANSFER_CONFIRMATION: {
                "primary": ["ì´ì²´í™•ì¸", "transfer confirmation", "ì†¡ê¸ˆí™•ì¸"],
                "secondary": ["ìŠ¹ì¸ë²ˆí˜¸", "ê³„ì¢Œë²ˆí˜¸", "ì†¡ê¸ˆê¸ˆì•¡", "approval", "account"],
                "negative": []
            }
        }
    
    def detect_document_type(self, text: str) -> Tuple[DocumentType, float]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ì„œ íƒ€ì… ê°ì§€
        
        Args:
            text: ì¶”ì¶œëœ í…ìŠ¤íŠ¸
            
        Returns:
            (ê°ì§€ëœ_ë¬¸ì„œ_íƒ€ì…, ì‹ ë¢°ë„)
        """
        
        if not text or len(text.strip()) < 20:
            return DocumentType.UNKNOWN, 0.0
        
        text_lower = text.lower()
        scores = {}
        
        # ê° ë¬¸ì„œ íƒ€ì…ë³„ ì ìˆ˜ ê³„ì‚°
        for doc_type, keywords in self.type_keywords.items():
            score = 0.0
            found_keywords = []
            
            # Primary í‚¤ì›Œë“œ ì ìˆ˜ (ê°€ì¤‘ì¹˜ 3)
            for keyword in keywords["primary"]:
                count = text_lower.count(keyword.lower())
                if count > 0:
                    score += count * 3
                    found_keywords.append(keyword)
            
            # Secondary í‚¤ì›Œë“œ ì ìˆ˜ (ê°€ì¤‘ì¹˜ 1)
            for keyword in keywords["secondary"]:
                count = text_lower.count(keyword.lower())
                if count > 0:
                    score += count * 1
                    found_keywords.append(keyword)
            
            # Negative í‚¤ì›Œë“œ íŒ¨ë„í‹° (ê°€ì¤‘ì¹˜ -2)
            for keyword in keywords["negative"]:
                count = text_lower.count(keyword.lower())
                if count > 0:
                    score -= count * 2
            
            scores[doc_type] = {
                "score": score,
                "found_keywords": found_keywords[:5]  # ìµœëŒ€ 5ê°œê¹Œì§€
            }
            
            if self.verbose and score > 0:
                logger.info(f"ğŸ“‹ {doc_type.value}: {score}ì  ({len(found_keywords)}ê°œ í‚¤ì›Œë“œ)")
        
        # ìµœê³  ì ìˆ˜ ë¬¸ì„œ íƒ€ì… ì„ íƒ
        if not scores or all(data["score"] <= 0 for data in scores.values()):
            return DocumentType.UNKNOWN, 0.0
        
        best_type = max(scores.items(), key=lambda x: x[1]["score"])
        doc_type, data = best_type
        
        # ì‹ ë¢°ë„ ê³„ì‚° (0~1)
        max_score = data["score"]
        total_keywords = len(self.type_keywords[doc_type]["primary"]) + len(self.type_keywords[doc_type]["secondary"])
        confidence = min(1.0, max_score / (total_keywords * 2))  # ì •ê·œí™”
        
        if self.verbose:
            logger.info(f"ğŸ¯ ê°ì§€ ê²°ê³¼: {doc_type.value} (ì‹ ë¢°ë„: {confidence:.2f})")
        
        return doc_type, confidence
    
    def detect_multiple_documents(self, text: str) -> List[Tuple[DocumentType, float, Tuple[int, int]]]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ë³µìˆ˜ ë¬¸ì„œ íƒ€ì… ê°ì§€ ë° ê°œë³„ ë¬¸ì„œ ë¶„ë¦¬
        
        Args:
            text: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ (í˜ì´ì§€ êµ¬ë¶„ì í¬í•¨)
            
        Returns:
            List[(ë¬¸ì„œ_íƒ€ì…, ì‹ ë¢°ë„, í˜ì´ì§€_ë²”ìœ„)]
        """
        
        # í˜ì´ì§€ë³„ë¡œ í…ìŠ¤íŠ¸ ë¶„ë¦¬
        pages = self._split_text_by_pages(text)
        
        # 1ë‹¨ê³„: í˜ì´ì§€ë³„ ë¬¸ì„œ íƒ€ì… ê°ì§€
        page_doc_types = []
        for page_num, page_text in enumerate(pages, 1):
            doc_type, confidence = self.detect_document_type(page_text)
            page_doc_types.append((page_num, doc_type, confidence, page_text))
        
        # 2ë‹¨ê³„: ë™ì¼í•œ ë¬¸ì„œ íƒ€ì… ë‚´ì—ì„œ ê°œë³„ ë¬¸ì„œ ë¶„ë¦¬
        detected_docs = []
        current_group = []
        
        for page_num, doc_type, confidence, page_text in page_doc_types:
            if not current_group or current_group[-1][1] == doc_type:
                # ê°™ì€ ë¬¸ì„œ íƒ€ì…ì´ë©´ ê·¸ë£¹ì— ì¶”ê°€
                current_group.append((page_num, doc_type, confidence, page_text))
            else:
                # ë‹¤ë¥¸ ë¬¸ì„œ íƒ€ì…ì´ë©´ ì´ì „ ê·¸ë£¹ ì²˜ë¦¬ í›„ ìƒˆ ê·¸ë£¹ ì‹œì‘
                if current_group:
                    individual_docs = self._split_individual_documents(current_group)
                    detected_docs.extend(individual_docs)
                current_group = [(page_num, doc_type, confidence, page_text)]
        
        # ë§ˆì§€ë§‰ ê·¸ë£¹ ì²˜ë¦¬
        if current_group:
            individual_docs = self._split_individual_documents(current_group)
            detected_docs.extend(individual_docs)
        
        if self.verbose:
            logger.info(f"ğŸ¯ ê°ì§€ëœ ê°œë³„ ë¬¸ì„œ: {len(detected_docs)}ê°œ")
            for i, (dtype, conf, pages) in enumerate(detected_docs):
                logger.info(f"  {i+1}. {dtype.value} (í˜ì´ì§€ {pages[0]}-{pages[1]}, ì‹ ë¢°ë„: {conf:.2f})")
        
        return detected_docs
    
    def _split_individual_documents(self, doc_group: List[Tuple[int, DocumentType, float, str]]) -> List[Tuple[DocumentType, float, Tuple[int, int]]]:
        """
        ë™ì¼í•œ ë¬¸ì„œ íƒ€ì… ê·¸ë£¹ì—ì„œ ê°œë³„ ë¬¸ì„œë“¤ì„ ë¶„ë¦¬
        
        Args:
            doc_group: [(í˜ì´ì§€ë²ˆí˜¸, ë¬¸ì„œíƒ€ì…, ì‹ ë¢°ë„, í…ìŠ¤íŠ¸), ...]
            
        Returns:
            List[(ë¬¸ì„œ_íƒ€ì…, ì‹ ë¢°ë„, í˜ì´ì§€_ë²”ìœ„)]
        """
        
        if not doc_group:
            return []
        
        doc_type = doc_group[0][1]
        individual_docs = []
        
        if doc_type == DocumentType.BILL_OF_LADING:
            # ì„ í•˜ì¦ê¶Œ: B/L ë²ˆí˜¸ë¡œ ë¶„ë¦¬
            bl_groups = self._group_by_bl_number(doc_group)
            for bl_number, pages_group in bl_groups.items():
                start_page = min(page[0] for page in pages_group)
                end_page = max(page[0] for page in pages_group)
                avg_confidence = sum(page[2] for page in pages_group) / len(pages_group)
                individual_docs.append((doc_type, avg_confidence, (start_page, end_page)))
                
        elif doc_type == DocumentType.EXPORT_DECLARATION:
            # ìˆ˜ì¶œì‹ ê³ í•„ì¦: ì‹ ê³ ë²ˆí˜¸ë¡œ ë¶„ë¦¬
            decl_groups = self._group_by_declaration_number(doc_group)
            for decl_number, pages_group in decl_groups.items():
                start_page = min(page[0] for page in pages_group)
                end_page = max(page[0] for page in pages_group)
                avg_confidence = sum(page[2] for page in pages_group) / len(pages_group)
                individual_docs.append((doc_type, avg_confidence, (start_page, end_page)))
                
        elif doc_type == DocumentType.TAX_INVOICE:
            # ì„¸ê¸ˆê³„ì‚°ì„œ: ì„¸ê¸ˆê³„ì‚°ì„œ ë²ˆí˜¸ë¡œ ë¶„ë¦¬
            tax_groups = self._group_by_tax_invoice_number(doc_group)
            for tax_number, pages_group in tax_groups.items():
                start_page = min(page[0] for page in pages_group)
                end_page = max(page[0] for page in pages_group)
                avg_confidence = sum(page[2] for page in pages_group) / len(pages_group)
                individual_docs.append((doc_type, avg_confidence, (start_page, end_page)))
                
        elif doc_type == DocumentType.INVOICE:
            # ì¸ë³´ì´ìŠ¤: ì¸ë³´ì´ìŠ¤ ë²ˆí˜¸ë¡œ ë¶„ë¦¬
            invoice_groups = self._group_by_invoice_number(doc_group)
            for invoice_number, pages_group in invoice_groups.items():
                start_page = min(page[0] for page in pages_group)
                end_page = max(page[0] for page in pages_group)
                avg_confidence = sum(page[2] for page in pages_group) / len(pages_group)
                individual_docs.append((doc_type, avg_confidence, (start_page, end_page)))
                
        else:
            # ê¸°íƒ€ ë¬¸ì„œ íƒ€ì…: ì—°ì†ëœ í˜ì´ì§€ë¡œ ì²˜ë¦¬
            start_page = doc_group[0][0]
            end_page = doc_group[-1][0]
            avg_confidence = sum(page[2] for page in doc_group) / len(doc_group)
            individual_docs.append((doc_type, avg_confidence, (start_page, end_page)))
        
        return individual_docs
    
    def _group_by_bl_number(self, doc_group: List[Tuple[int, DocumentType, float, str]]) -> Dict[str, List[Tuple[int, DocumentType, float, str]]]:
        """B/L ë²ˆí˜¸ë¡œ í˜ì´ì§€ë“¤ì„ ê·¸ë£¹í™”"""
        
        bl_patterns = [
            re.compile(r'b/?l\s*(?:no\.?)?\s*:?\s*([A-Z]{2,4}\d{6,12})', re.IGNORECASE),
            re.compile(r'bill\s*of\s*lading\s*(?:no\.?)?\s*:?\s*([A-Z]{2,4}\d{6,12})', re.IGNORECASE),
            re.compile(r'([A-Z]{2,4}\d{6,12})', re.IGNORECASE)  # ì¼ë°˜ì ì¸ B/L ë²ˆí˜¸ íŒ¨í„´
        ]
        
        groups = {}
        unknown_count = 1
        
        for page_info in doc_group:
            page_num, doc_type, confidence, page_text = page_info
            bl_number = None
            
            # B/L ë²ˆí˜¸ ì°¾ê¸°
            for pattern in bl_patterns:
                matches = pattern.findall(page_text)
                if matches:
                    # ê°€ì¥ ì•ì— ë‚˜ì˜¤ëŠ” B/L ë²ˆí˜¸ ì‚¬ìš©
                    bl_number = matches[0]
                    break
            
            # B/L ë²ˆí˜¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
            if not bl_number:
                bl_number = f"UNKNOWN_BL_{unknown_count}"
                unknown_count += 1
            
            if bl_number not in groups:
                groups[bl_number] = []
            groups[bl_number].append(page_info)
        
        return groups
    
    def _group_by_declaration_number(self, doc_group: List[Tuple[int, DocumentType, float, str]]) -> Dict[str, List[Tuple[int, DocumentType, float, str]]]:
        """ì‹ ê³ ë²ˆí˜¸ë¡œ í˜ì´ì§€ë“¤ì„ ê·¸ë£¹í™”"""
        
        decl_patterns = [
            re.compile(r'ì‹ ê³ ë²ˆí˜¸\s*([0-9]{5}-[0-9]{2}-[0-9]{6}[A-Z]?)', re.IGNORECASE),
            re.compile(r'(\d{5}-\d{2}-\d{6}[A-Z]?)', re.IGNORECASE)
        ]
        
        groups = {}
        unknown_count = 1
        
        for page_info in doc_group:
            page_num, doc_type, confidence, page_text = page_info
            decl_number = None
            
            # ì‹ ê³ ë²ˆí˜¸ ì°¾ê¸°
            for pattern in decl_patterns:
                matches = pattern.findall(page_text)
                if matches:
                    decl_number = matches[0]
                    break
            
            # ì‹ ê³ ë²ˆí˜¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
            if not decl_number:
                decl_number = f"UNKNOWN_DECL_{unknown_count}"
                unknown_count += 1
            
            if decl_number not in groups:
                groups[decl_number] = []
            groups[decl_number].append(page_info)
        
        return groups
    
    def _group_by_tax_invoice_number(self, doc_group: List[Tuple[int, DocumentType, float, str]]) -> Dict[str, List[Tuple[int, DocumentType, float, str]]]:
        """ì„¸ê¸ˆê³„ì‚°ì„œ ë²ˆí˜¸ë¡œ í˜ì´ì§€ë“¤ì„ ê·¸ë£¹í™”"""
        
        tax_patterns = [
            re.compile(r'ì„¸ê¸ˆê³„ì‚°ì„œ.*?ë²ˆí˜¸.*?([0-9-]+)', re.IGNORECASE),
            re.compile(r'tax\s*invoice.*?no.*?([0-9-]+)', re.IGNORECASE)
        ]
        
        groups = {}
        unknown_count = 1
        
        for page_info in doc_group:
            page_num, doc_type, confidence, page_text = page_info
            tax_number = None
            
            # ì„¸ê¸ˆê³„ì‚°ì„œ ë²ˆí˜¸ ì°¾ê¸°
            for pattern in tax_patterns:
                matches = pattern.findall(page_text)
                if matches:
                    tax_number = matches[0]
                    break
            
            # ì„¸ê¸ˆê³„ì‚°ì„œ ë²ˆí˜¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
            if not tax_number:
                tax_number = f"UNKNOWN_TAX_{unknown_count}"
                unknown_count += 1
            
            if tax_number not in groups:
                groups[tax_number] = []
            groups[tax_number].append(page_info)
        
        return groups
    
    def _group_by_invoice_number(self, doc_group: List[Tuple[int, DocumentType, float, str]]) -> Dict[str, List[Tuple[int, DocumentType, float, str]]]:
        """ì¸ë³´ì´ìŠ¤ ë²ˆí˜¸ë¡œ í˜ì´ì§€ë“¤ì„ ê·¸ë£¹í™”"""
        
        invoice_patterns = [
            re.compile(r'invoice\s*(?:no\.?)?\s*:?\s*([A-Z0-9-]+)', re.IGNORECASE),
            re.compile(r'commercial\s*invoice\s*(?:no\.?)?\s*:?\s*([A-Z0-9-]+)', re.IGNORECASE)
        ]
        
        groups = {}
        unknown_count = 1
        
        for page_info in doc_group:
            page_num, doc_type, confidence, page_text = page_info
            invoice_number = None
            
            # ì¸ë³´ì´ìŠ¤ ë²ˆí˜¸ ì°¾ê¸°
            for pattern in invoice_patterns:
                matches = pattern.findall(page_text)
                if matches:
                    invoice_number = matches[0]
                    break
            
            # ì¸ë³´ì´ìŠ¤ ë²ˆí˜¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
            if not invoice_number:
                invoice_number = f"UNKNOWN_INV_{unknown_count}"
                unknown_count += 1
            
            if invoice_number not in groups:
                groups[invoice_number] = []
            groups[invoice_number].append(page_info)
        
        return groups
    
    def _split_text_by_pages(self, text: str) -> List[str]:
        """í˜ì´ì§€ êµ¬ë¶„ìë¡œ í…ìŠ¤íŠ¸ ë¶„ë¦¬"""
        
        # í˜ì´ì§€ êµ¬ë¶„ì íŒ¨í„´ë“¤
        page_patterns = [
            r'--- í˜ì´ì§€ (\d+) ---',
            r'Page (\d+)',
            r'\f',  # Form feed character
        ]
        
        # í˜ì´ì§€ êµ¬ë¶„ìë¡œ ë¶„ë¦¬
        pages = []
        current_text = text
        
        for pattern in page_patterns:
            if re.search(pattern, current_text):
                parts = re.split(pattern, current_text)
                # ì²« ë²ˆì§¸ ë¶€ë¶„ (í˜ì´ì§€ êµ¬ë¶„ì ì „)
                if parts[0].strip():
                    pages.append(parts[0].strip())
                
                # ë‚˜ë¨¸ì§€ ë¶€ë¶„ë“¤ì„ í˜ì´ì§€ë¡œ ì²˜ë¦¬
                for i in range(1, len(parts), 2):  # í™€ìˆ˜ ì¸ë±ìŠ¤ëŠ” í˜ì´ì§€ ë²ˆí˜¸, ì§ìˆ˜ëŠ” ë‚´ìš©
                    if i + 1 < len(parts) and parts[i + 1].strip():
                        pages.append(parts[i + 1].strip())
                break
        
        # í˜ì´ì§€ êµ¬ë¶„ìê°€ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ í˜ì´ì§€ë¡œ ì²˜ë¦¬
        if not pages:
            pages = [text]
        
        return pages
    
    def _calculate_final_confidence(self, text: str, doc_type: DocumentType) -> float:
        """ìµœì¢… ì‹ ë¢°ë„ ê³„ì‚°"""
        _, confidence = self.detect_document_type(text)
        return confidence
    
    def get_detection_details(self, text: str) -> Dict[str, Any]:
        """ìƒì„¸í•œ ë¬¸ì„œ íƒ€ì… ê°ì§€ ì •ë³´ ë°˜í™˜"""
        
        text_lower = text.lower()
        details = {}
        
        for doc_type, keywords in self.type_keywords.items():
            found_primary = [kw for kw in keywords["primary"] if kw.lower() in text_lower]
            found_secondary = [kw for kw in keywords["secondary"] if kw.lower() in text_lower]
            found_negative = [kw for kw in keywords["negative"] if kw.lower() in text_lower]
            
            score = len(found_primary) * 3 + len(found_secondary) * 1 - len(found_negative) * 2
            
            details[doc_type.value] = {
                "score": score,
                "found_primary_keywords": found_primary,
                "found_secondary_keywords": found_secondary,
                "found_negative_keywords": found_negative,
                "total_keywords_found": len(found_primary) + len(found_secondary)
            }
        
        return details


class PDFProcessor:
    """
    ì¢…í•© PDF ì²˜ë¦¬ê¸°
    
    íŒŒì‹± ì—”ì§„ê³¼ ë¬¸ì„œ íƒ€ì… ê°ì§€ê¸°ë¥¼ ê²°í•©í•œ í†µí•© ì¸í„°í˜ì´ìŠ¤
    """
    
    def __init__(self, upstage_api_key: str = None, verbose: bool = False):
        self.parser = PDFParsingEngine(upstage_api_key, verbose)
        self.detector = DocumentTypeDetector(verbose)
        self.verbose = verbose
    
    async def process_pdf(
        self, 
        file_path: str,
        preferred_engine: ExtractionEngine = ExtractionEngine.UPSTAGE
    ) -> PDFProcessingResult:
        """
        PDF íŒŒì¼ ì™„ì „ ì²˜ë¦¬
        
        Args:
            file_path: PDF íŒŒì¼ ê²½ë¡œ
            preferred_engine: ì„ í˜¸í•˜ëŠ” ì¶”ì¶œ ì—”ì§„
            
        Returns:
            PDFProcessingResult ê°ì²´
        """
        
        start_time = datetime.now()
        file_info = get_file_info(file_path)
        
        # PDF í˜ì´ì§€ ìˆ˜ í™•ì¸
        try:
            doc = fitz.open(file_path)
            total_pages = len(doc)
            doc.close()
        except:
            total_pages = 1  # ê¸°ë³¸ê°’
        
        # ê²°ê³¼ ê°ì²´ ì´ˆê¸°í™”
        result = PDFProcessingResult(
            file_path=file_path,
            file_name=file_info["stem"],
            file_size_mb=file_info["size_mb"],
            total_pages=total_pages,
            processing_start_time=start_time,
            status=ProcessingStatus.PROCESSING
        )
        
        try:
            if self.verbose:
                logger.info(f"ğŸš€ PDF ì¢…í•© ì²˜ë¦¬ ì‹œì‘: {file_info['name']}")
            
            # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
            extracted_text, used_engine, parsing_time = await self.parser.extract_text_from_pdf(
                file_path, preferred_engine
            )
            
            result.extraction_engines_used.append(used_engine)
            result.primary_engine = used_engine
            
            # 2. ë³µìˆ˜ ë¬¸ì„œ íƒ€ì… ê°ì§€
            multiple_docs = self.detector.detect_multiple_documents(extracted_text)
            
            # 3. ë¬¸ì„œ ê°ì§€ ê²°ê³¼ ìƒì„±
            detections = []
            for doc_type, confidence, page_range in multiple_docs:
                # í•´ë‹¹ í˜ì´ì§€ ë²”ìœ„ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                page_text = self._extract_text_for_page_range(extracted_text, page_range)
                
                detection = DocumentDetection(
                    document_type=doc_type,
                    confidence=confidence,
                    page_range=page_range,
                    key_indicators=self._get_key_indicators(page_text, doc_type),
                    extracted_data={"raw_text": page_text}
                )
                detections.append(detection)
            
            # ê°ì§€ëœ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ë‹¨ì¼ ë¬¸ì„œë¡œ ì²˜ë¦¬
            if not detections:
                doc_type, confidence = self.detector.detect_document_type(extracted_text)
                detection = DocumentDetection(
                    document_type=doc_type,
                    confidence=confidence,
                    page_range=(1, result.total_pages or 1),
                    key_indicators=self._get_key_indicators(extracted_text, doc_type),
                    extracted_data={"raw_text": extracted_text}
                )
                detections = [detection]
            
            result.detected_documents = detections
            # ì£¼ ë¬¸ì„œ íƒ€ì…ì€ ì²« ë²ˆì§¸ ë˜ëŠ” ê°€ì¥ ì‹ ë¢°ë„ê°€ ë†’ì€ ë¬¸ì„œë¡œ ì„¤ì •
            result.primary_document_type = max(detections, key=lambda x: x.confidence).document_type
            result.status = ProcessingStatus.COMPLETED
            
            if self.verbose:
                logger.info(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {doc_type.value} (ì—”ì§„: {used_engine.value})")
            
        except Exception as e:
            result.add_error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            result.status = ProcessingStatus.FAILED
            
            if self.verbose:
                logger.error(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        end_time = datetime.now()
        result.processing_end_time = end_time
        result.processing_duration_seconds = (end_time - start_time).total_seconds()
        
        return result
    
    def _extract_text_for_page_range(self, full_text: str, page_range: Tuple[int, int]) -> str:
        """í˜ì´ì§€ ë²”ìœ„ì— í•´ë‹¹í•˜ëŠ” í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        start_page, end_page = page_range
        
        # í˜ì´ì§€ êµ¬ë¶„ìë¡œ ë¶„ë¦¬
        pages = self.detector._split_text_by_pages(full_text)
        
        # ì¸ë±ìŠ¤ ì¡°ì • (1-based -> 0-based)
        start_idx = max(0, start_page - 1)
        end_idx = min(len(pages), end_page)
        
        # í•´ë‹¹ ë²”ìœ„ì˜ í˜ì´ì§€ë“¤ ê²°í•©
        selected_pages = pages[start_idx:end_idx]
        return "\n\n".join(selected_pages)
    
    def _get_key_indicators(self, text: str, doc_type: DocumentType) -> List[str]:
        """ë¬¸ì„œ íƒ€ì… ê°ì§€ì— ì‚¬ìš©ëœ í•µì‹¬ í‚¤ì›Œë“œ ë°˜í™˜"""
        
        details = self.detector.get_detection_details(text)
        type_details = details.get(doc_type.value, {})
        
        indicators = []
        indicators.extend(type_details.get("found_primary_keywords", []))
        indicators.extend(type_details.get("found_secondary_keywords", [])[:3])  # ìµœëŒ€ 3ê°œ
        
        return indicators[:5]  # ìµœëŒ€ 5ê°œ ë°˜í™˜
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        
        engine_stats = self.parser.get_engine_statistics()
        
        return {
            "engine_statistics": engine_stats,
            "supported_document_types": [dt.value for dt in DocumentType],
            "available_engines": [engine.value for engine in ExtractionEngine]
        }